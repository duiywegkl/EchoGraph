from typing import List, Dict, Any, Optional, Iterator
import openai
from loguru import logger
from src.utils.config import config

class LLMClient:
    def __init__(self):
        self.client = openai.OpenAI(
            api_key=config.llm.api_key,
            base_url=config.llm.base_url
        )
        self.model = config.llm.model
        self.max_tokens = config.llm.max_tokens
        self.temperature = config.llm.temperature

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """å•æ¬¡LLMè°ƒç”¨ - ä¸¥æ ¼JSONæ¨¡å¼ï¼Œå¸¦è¯¦ç»†æ—¥å¿—"""
        try:
            # è®°å½•è°ƒç”¨å‚æ•°å’Œå…³é”®ä¿¡æ¯ï¼ˆä¸è®°å½•APIå¯†é’¥ï¼‰
            try:
                sys_msgs = [m for m in messages if m.get('role') == 'system']
                usr_msgs = [m for m in messages if m.get('role') == 'user']
                logger.info(f"ğŸ¤– [LLM] è°ƒç”¨å¼€å§‹ | model={kwargs.get('model', self.model)} | max_tokens={kwargs.get('max_tokens', self.max_tokens)} | temp={kwargs.get('temperature', self.temperature)}")
                if sys_msgs:
                    logger.info(f"[LLM] System message preview (first 300 chars):\n---\n{sys_msgs[-1].get('content','')[:300]}\n---")
                if usr_msgs:
                    logger.info(f"[LLM] User prompt preview (first 500 chars):\n---\n{usr_msgs[-1].get('content','')[:500]}\n---")
                logger.debug(f"[LLM] Full messages: {messages}")
            except Exception:
                pass

            response = self.client.chat.completions.create(
                model=kwargs.get('model', self.model),
                messages=messages,
                max_tokens=kwargs.get('max_tokens', self.max_tokens),
                temperature=kwargs.get('temperature', self.temperature),
                timeout=config.llm.request_timeout,
                response_format={"type": "json_object"} # å¯ç”¨JSONæ¨¡å¼
            )
            content = response.choices[0].message.content or ""
            logger.info(f"ğŸ¤– [LLM] è°ƒç”¨æˆåŠŸ | è¿”å›é•¿åº¦={len(content)}")
            logger.info(f"[LLM] Raw response preview (first 800 chars):\n---\n{content[:800]}\n---")
            logger.debug(f"[LLM] Full raw response object: {response}")
            return content
        except Exception as e:
            logger.error(f"âŒ [LLM] è°ƒç”¨å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶æ— æ³•å“åº”ã€‚"

    def generate_response(self, prompt: str, max_tokens: int = None, temperature: float = None, system_message: str = None) -> str:
        """
        å…¼å®¹GRAG Agentè°ƒç”¨çš„ç»Ÿä¸€æ¥å£
        å°†å•ä¸ªpromptè½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼è¿›è¡Œè°ƒç”¨
        """
        messages = []

        if system_message:
            messages.append({"role": "system", "content": system_message})

        messages.append({"role": "user", "content": prompt})

        return self.chat(
            messages=messages,
            max_tokens=max_tokens or self.max_tokens,
            temperature=temperature or self.temperature
        )
